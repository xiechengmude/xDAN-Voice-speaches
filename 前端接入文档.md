# Speaches è¯­éŸ³è½¬æ–‡å­—å‰ç«¯æ¥å…¥æ–‡æ¡£

## ç›®å½•
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [éå®æ—¶è¯­éŸ³è½¬æ–‡å­—](#éå®æ—¶è¯­éŸ³è½¬æ–‡å­—)
- [å®æ—¶è¯­éŸ³è½¬æ–‡å­—](#å®æ—¶è¯­éŸ³è½¬æ–‡å­—)
- [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)
- [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## å¿«é€Ÿå¼€å§‹

### æœåŠ¡åœ°å€
```javascript
const SPEACHES_BASE_URL = 'http://localhost:8000';  // æœ¬åœ°æœåŠ¡
// const SPEACHES_BASE_URL = 'https://your-api.com'; // ç”Ÿäº§ç¯å¢ƒ
```

### æ¨èæ¨¡å‹
```javascript
// ASR (è¯­éŸ³è¯†åˆ«) æ¨¡å‹
const ASR_MODEL = 'Systran/faster-distil-whisper-large-v3';  // æ¨èï¼šé€Ÿåº¦å¿«ï¼Œå‡†ç¡®ç‡é«˜

// è¯­è¨€è®¾ç½®
const LANGUAGE = 'zh';  // ä¸­æ–‡
// const LANGUAGE = 'en';  // è‹±æ–‡
// const LANGUAGE = '';    // è‡ªåŠ¨æ£€æµ‹
```

## éå®æ—¶è¯­éŸ³è½¬æ–‡å­—

### 1. åŸºç¡€å®ç°ï¼ˆç±»ä¼¼å¾®ä¿¡è¯­éŸ³è¾“å…¥ï¼‰

```html
<!DOCTYPE html>
<html>
<head>
    <title>è¯­éŸ³è½¬æ–‡å­—</title>
</head>
<body>
    <button id="recordBtn">æŒ‰ä½è¯´è¯</button>
    <div id="result"></div>

    <script>
    class VoiceToText {
        constructor() {
            this.mediaRecorder = null;
            this.audioChunks = [];
            this.isRecording = false;
            
            // é…ç½®
            this.config = {
                baseURL: 'http://localhost:8000',
                model: 'Systran/faster-distil-whisper-large-v3',
                language: 'zh'
            };
            
            this.init();
        }
        
        init() {
            const btn = document.getElementById('recordBtn');
            
            // é¼ æ ‡äº‹ä»¶
            btn.addEventListener('mousedown', () => this.startRecording());
            btn.addEventListener('mouseup', () => this.stopRecording());
            btn.addEventListener('mouseleave', () => {
                if (this.isRecording) this.stopRecording();
            });
            
            // è§¦æ‘¸äº‹ä»¶ï¼ˆç§»åŠ¨ç«¯ï¼‰
            btn.addEventListener('touchstart', (e) => {
                e.preventDefault();
                this.startRecording();
            });
            btn.addEventListener('touchend', (e) => {
                e.preventDefault();
                this.stopRecording();
            });
        }
        
        async startRecording() {
            try {
                // è·å–éº¦å…‹é£æƒé™
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        channelCount: 1,
                        sampleRate: 16000
                    } 
                });
                
                this.audioChunks = [];
                this.mediaRecorder = new MediaRecorder(stream);
                
                this.mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        this.audioChunks.push(event.data);
                    }
                };
                
                this.mediaRecorder.onstop = () => {
                    this.transcribeAudio();
                };
                
                this.mediaRecorder.start();
                this.isRecording = true;
                
                document.getElementById('recordBtn').textContent = 'æ¾å¼€ç»“æŸ';
                
            } catch (err) {
                console.error('å½•éŸ³å¤±è´¥:', err);
                alert('æ— æ³•è®¿é—®éº¦å…‹é£');
            }
        }
        
        stopRecording() {
            if (this.mediaRecorder && this.isRecording) {
                this.mediaRecorder.stop();
                this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
                this.isRecording = false;
                
                document.getElementById('recordBtn').textContent = 'æŒ‰ä½è¯´è¯';
            }
        }
        
        async transcribeAudio() {
            // åˆ›å»ºéŸ³é¢‘ Blob
            const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
            
            // å‡†å¤‡è¡¨å•æ•°æ®
            const formData = new FormData();
            formData.append('file', audioBlob, 'recording.webm');
            formData.append('model', this.config.model);
            formData.append('language', this.config.language);
            formData.append('response_format', 'json');
            
            try {
                document.getElementById('result').textContent = 'è¯†åˆ«ä¸­...';
                
                const response = await fetch(`${this.config.baseURL}/v1/audio/transcriptions`, {
                    method: 'POST',
                    body: formData
                });
                
                if (response.ok) {
                    const result = await response.json();
                    document.getElementById('result').textContent = result.text || '(æ— æ³•è¯†åˆ«)';
                } else {
                    throw new Error(`HTTP ${response.status}`);
                }
                
            } catch (err) {
                console.error('è½¬å½•å¤±è´¥:', err);
                document.getElementById('result').textContent = 'è¯†åˆ«å¤±è´¥';
            }
        }
    }
    
    // åˆå§‹åŒ–
    new VoiceToText();
    </script>
</body>
</html>
```

### 2. é«˜çº§å®ç°ï¼ˆå¸¦ UI åé¦ˆï¼‰

```javascript
// voice-input.js
class AdvancedVoiceInput {
    constructor(options = {}) {
        this.options = {
            baseURL: 'http://localhost:8000',
            model: 'Systran/faster-distil-whisper-large-v3',
            language: 'zh',
            maxDuration: 60000,  // æœ€é•¿å½•éŸ³æ—¶é—´ 60 ç§’
            ...options
        };
        
        this.callbacks = {
            onStart: null,
            onStop: null,
            onResult: null,
            onError: null
        };
        
        this.audioContext = null;
        this.analyser = null;
    }
    
    // è®¾ç½®å›è°ƒ
    on(event, callback) {
        if (this.callbacks.hasOwnProperty(event)) {
            this.callbacks[event] = callback;
        }
    }
    
    // å¼€å§‹å½•éŸ³
    async startRecording() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    sampleRate: 16000
                } 
            });
            
            // éŸ³é¢‘å¯è§†åŒ–
            this.setupAudioVisualization(stream);
            
            // è®¾ç½® MediaRecorder
            this.audioChunks = [];
            this.mediaRecorder = new MediaRecorder(stream, {
                mimeType: 'audio/webm;codecs=opus'
            });
            
            this.mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    this.audioChunks.push(event.data);
                }
            };
            
            this.mediaRecorder.onstop = () => {
                this.handleRecordingComplete();
            };
            
            // å¼€å§‹å½•éŸ³
            this.mediaRecorder.start();
            this.startTime = Date.now();
            
            // è®¾ç½®æœ€å¤§å½•éŸ³æ—¶é•¿
            this.maxDurationTimeout = setTimeout(() => {
                this.stopRecording();
            }, this.options.maxDuration);
            
            // è§¦å‘å›è°ƒ
            this.callbacks.onStart?.();
            
        } catch (err) {
            this.callbacks.onError?.(err);
        }
    }
    
    // åœæ­¢å½•éŸ³
    stopRecording() {
        if (this.mediaRecorder && this.mediaRecorder.state === 'recording') {
            clearTimeout(this.maxDurationTimeout);
            
            this.mediaRecorder.stop();
            this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
            
            // åœæ­¢éŸ³é¢‘åˆ†æ
            if (this.audioContext) {
                this.audioContext.close();
            }
            
            const duration = (Date.now() - this.startTime) / 1000;
            this.callbacks.onStop?.(duration);
        }
    }
    
    // å¤„ç†å½•éŸ³å®Œæˆ
    async handleRecordingComplete() {
        const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
        
        // è½¬æ¢ä¸º WAVï¼ˆå¯é€‰ï¼Œæé«˜å…¼å®¹æ€§ï¼‰
        // const wavBlob = await this.convertToWav(audioBlob);
        
        // å‘é€åˆ°æœåŠ¡å™¨
        await this.transcribe(audioBlob);
    }
    
    // è¯­éŸ³è½¬æ–‡å­—
    async transcribe(audioBlob) {
        const formData = new FormData();
        formData.append('file', audioBlob, 'recording.webm');
        formData.append('model', this.options.model);
        formData.append('language', this.options.language);
        formData.append('response_format', 'verbose_json');  // è·å–è¯¦ç»†ä¿¡æ¯
        
        try {
            const response = await fetch(`${this.options.baseURL}/v1/audio/transcriptions`, {
                method: 'POST',
                body: formData
            });
            
            if (!response.ok) {
                throw new Error(`HTTP ${response.status}`);
            }
            
            const result = await response.json();
            
            // å¤„ç†ç»“æœ
            this.callbacks.onResult?.({
                text: result.text,
                language: result.language,
                duration: result.duration,
                words: result.words  // è¯çº§æ—¶é—´æˆ³
            });
            
        } catch (err) {
            this.callbacks.onError?.(err);
        }
    }
    
    // éŸ³é¢‘å¯è§†åŒ–
    setupAudioVisualization(stream) {
        this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const source = this.audioContext.createMediaStreamSource(stream);
        
        this.analyser = this.audioContext.createAnalyser();
        this.analyser.fftSize = 256;
        
        source.connect(this.analyser);
        
        // è¿”å›åˆ†æå™¨ä¾› UI ä½¿ç”¨
        return this.analyser;
    }
    
    // è·å–éŸ³é¢‘ç”µå¹³
    getAudioLevel() {
        if (!this.analyser) return 0;
        
        const dataArray = new Uint8Array(this.analyser.frequencyBinCount);
        this.analyser.getByteFrequencyData(dataArray);
        
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        return average / 255;  // å½’ä¸€åŒ–åˆ° 0-1
    }
}

// ä½¿ç”¨ç¤ºä¾‹
const voiceInput = new AdvancedVoiceInput({
    language: 'zh',
    maxDuration: 30000  // 30ç§’
});

// è®¾ç½®å›è°ƒ
voiceInput.on('onStart', () => {
    console.log('å¼€å§‹å½•éŸ³');
    updateUI('recording');
});

voiceInput.on('onStop', (duration) => {
    console.log(`å½•éŸ³ç»“æŸï¼Œæ—¶é•¿: ${duration}ç§’`);
    updateUI('processing');
});

voiceInput.on('onResult', (result) => {
    console.log('è¯†åˆ«ç»“æœ:', result.text);
    displayResult(result);
});

voiceInput.on('onError', (err) => {
    console.error('é”™è¯¯:', err);
    showError(err.message);
});
```

### 3. React ç»„ä»¶å®ç°

```jsx
// VoiceInput.jsx
import React, { useState, useRef } from 'react';

const VoiceInput = ({ onTranscription }) => {
    const [isRecording, setIsRecording] = useState(false);
    const [transcription, setTranscription] = useState('');
    const [isProcessing, setIsProcessing] = useState(false);
    const mediaRecorderRef = useRef(null);
    const audioChunksRef = useRef([]);
    
    const config = {
        baseURL: process.env.REACT_APP_SPEACHES_URL || 'http://localhost:8000',
        model: 'Systran/faster-distil-whisper-large-v3',
        language: 'zh'
    };
    
    const startRecording = async () => {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            
            audioChunksRef.current = [];
            mediaRecorderRef.current = new MediaRecorder(stream);
            
            mediaRecorderRef.current.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    audioChunksRef.current.push(event.data);
                }
            };
            
            mediaRecorderRef.current.onstop = handleRecordingComplete;
            
            mediaRecorderRef.current.start();
            setIsRecording(true);
            
        } catch (err) {
            console.error('Failed to start recording:', err);
            alert('æ— æ³•è®¿é—®éº¦å…‹é£');
        }
    };
    
    const stopRecording = () => {
        if (mediaRecorderRef.current && isRecording) {
            mediaRecorderRef.current.stop();
            mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
            setIsRecording(false);
        }
    };
    
    const handleRecordingComplete = async () => {
        setIsProcessing(true);
        
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        
        const formData = new FormData();
        formData.append('file', audioBlob, 'recording.webm');
        formData.append('model', config.model);
        formData.append('language', config.language);
        
        try {
            const response = await fetch(`${config.baseURL}/v1/audio/transcriptions`, {
                method: 'POST',
                body: formData
            });
            
            if (response.ok) {
                const result = await response.json();
                setTranscription(result.text);
                onTranscription?.(result.text);
            } else {
                throw new Error('Transcription failed');
            }
            
        } catch (err) {
            console.error('Transcription error:', err);
            setTranscription('è¯†åˆ«å¤±è´¥');
        } finally {
            setIsProcessing(false);
        }
    };
    
    return (
        <div className="voice-input">
            <button
                onMouseDown={startRecording}
                onMouseUp={stopRecording}
                onMouseLeave={stopRecording}
                onTouchStart={startRecording}
                onTouchEnd={stopRecording}
                disabled={isProcessing}
                className={`voice-button ${isRecording ? 'recording' : ''}`}
            >
                {isRecording ? 'æ¾å¼€ç»“æŸ' : 'æŒ‰ä½è¯´è¯'}
            </button>
            
            {isProcessing && <div className="processing">è¯†åˆ«ä¸­...</div>}
            
            {transcription && (
                <div className="transcription">
                    {transcription}
                </div>
            )}
        </div>
    );
};

export default VoiceInput;
```

### 4. Vue 3 ç»„ä»¶å®ç°

```vue
<!-- VoiceInput.vue -->
<template>
  <div class="voice-input">
    <button
      @mousedown="startRecording"
      @mouseup="stopRecording"
      @mouseleave="handleMouseLeave"
      @touchstart.prevent="startRecording"
      @touchend.prevent="stopRecording"
      :disabled="isProcessing"
      :class="['voice-button', { recording: isRecording }]"
    >
      {{ buttonText }}
    </button>
    
    <div v-if="isProcessing" class="processing">è¯†åˆ«ä¸­...</div>
    
    <div v-if="transcription" class="transcription">
      {{ transcription }}
    </div>
  </div>
</template>

<script setup>
import { ref, computed } from 'vue';

const emit = defineEmits(['transcription']);

const isRecording = ref(false);
const isProcessing = ref(false);
const transcription = ref('');
const mediaRecorder = ref(null);
const audioChunks = ref([]);

const config = {
  baseURL: import.meta.env.VITE_SPEACHES_URL || 'http://localhost:8000',
  model: 'Systran/faster-distil-whisper-large-v3',
  language: 'zh'
};

const buttonText = computed(() => {
  if (isRecording.value) return 'æ¾å¼€ç»“æŸ';
  if (isProcessing.value) return 'å¤„ç†ä¸­...';
  return 'æŒ‰ä½è¯´è¯';
});

const startRecording = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    
    audioChunks.value = [];
    mediaRecorder.value = new MediaRecorder(stream);
    
    mediaRecorder.value.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunks.value.push(event.data);
      }
    };
    
    mediaRecorder.value.onstop = handleRecordingComplete;
    
    mediaRecorder.value.start();
    isRecording.value = true;
    
  } catch (err) {
    console.error('å½•éŸ³å¤±è´¥:', err);
    alert('æ— æ³•è®¿é—®éº¦å…‹é£');
  }
};

const stopRecording = () => {
  if (mediaRecorder.value && isRecording.value) {
    mediaRecorder.value.stop();
    mediaRecorder.value.stream.getTracks().forEach(track => track.stop());
    isRecording.value = false;
  }
};

const handleMouseLeave = () => {
  if (isRecording.value) {
    stopRecording();
  }
};

const handleRecordingComplete = async () => {
  isProcessing.value = true;
  
  const audioBlob = new Blob(audioChunks.value, { type: 'audio/webm' });
  
  const formData = new FormData();
  formData.append('file', audioBlob, 'recording.webm');
  formData.append('model', config.model);
  formData.append('language', config.language);
  
  try {
    const response = await fetch(`${config.baseURL}/v1/audio/transcriptions`, {
      method: 'POST',
      body: formData
    });
    
    if (response.ok) {
      const result = await response.json();
      transcription.value = result.text;
      emit('transcription', result.text);
    } else {
      throw new Error('è½¬å½•å¤±è´¥');
    }
    
  } catch (err) {
    console.error('è½¬å½•é”™è¯¯:', err);
    transcription.value = 'è¯†åˆ«å¤±è´¥';
  } finally {
    isProcessing.value = false;
  }
};
</script>

<style scoped>
.voice-button {
  width: 120px;
  height: 120px;
  border-radius: 50%;
  background: #4CAF50;
  color: white;
  border: none;
  font-size: 16px;
  cursor: pointer;
  transition: all 0.3s;
}

.voice-button:hover {
  background: #45a049;
  transform: scale(1.05);
}

.voice-button.recording {
  background: #f44336;
  animation: pulse 1.5s infinite;
}

.voice-button:disabled {
  background: #ccc;
  cursor: not-allowed;
}

@keyframes pulse {
  0% {
    box-shadow: 0 0 0 0 rgba(244, 67, 54, 0.7);
  }
  70% {
    box-shadow: 0 0 0 20px rgba(244, 67, 54, 0);
  }
  100% {
    box-shadow: 0 0 0 0 rgba(244, 67, 54, 0);
  }
}

.processing {
  margin-top: 20px;
  color: #666;
}

.transcription {
  margin-top: 20px;
  padding: 15px;
  background: #f5f5f5;
  border-radius: 8px;
  min-height: 50px;
}
</style>
```

## å®æ—¶è¯­éŸ³è½¬æ–‡å­—

### 1. WebSocket å®æ—¶è¯†åˆ«

```javascript
// realtime-voice.js
class RealtimeVoiceRecognition {
    constructor(options = {}) {
        this.options = {
            wsURL: 'ws://localhost:8000/v1/realtime',
            model: 'Systran/faster-distil-whisper-large-v3',
            language: 'zh',
            ...options
        };
        
        this.ws = null;
        this.mediaRecorder = null;
        this.isConnected = false;
    }
    
    // è¿æ¥ WebSocket
    async connect() {
        return new Promise((resolve, reject) => {
            this.ws = new WebSocket(this.options.wsURL);
            
            this.ws.onopen = () => {
                console.log('WebSocket è¿æ¥æˆåŠŸ');
                this.isConnected = true;
                
                // é…ç½®ä¼šè¯
                this.ws.send(JSON.stringify({
                    type: 'session.update',
                    session: {
                        model: this.options.model,
                        transcription_options: {
                            language: this.options.language,
                            temperature: 0.3,
                            beam_size: 5
                        }
                    }
                }));
                
                resolve();
            };
            
            this.ws.onmessage = (event) => {
                const data = JSON.parse(event.data);
                this.handleMessage(data);
            };
            
            this.ws.onerror = (error) => {
                console.error('WebSocket é”™è¯¯:', error);
                reject(error);
            };
            
            this.ws.onclose = () => {
                console.log('WebSocket è¿æ¥å…³é—­');
                this.isConnected = false;
            };
        });
    }
    
    // å¼€å§‹å®æ—¶è¯†åˆ«
    async startStreaming() {
        if (!this.isConnected) {
            await this.connect();
        }
        
        const stream = await navigator.mediaDevices.getUserMedia({ 
            audio: {
                channelCount: 1,
                sampleRate: 16000,
                echoCancellation: true,
                noiseSuppression: true
            } 
        });
        
        // ä½¿ç”¨ MediaRecorder æ•è·éŸ³é¢‘
        this.mediaRecorder = new MediaRecorder(stream, {
            mimeType: 'audio/webm;codecs=opus'
        });
        
        this.mediaRecorder.ondataavailable = async (event) => {
            if (event.data.size > 0 && this.ws.readyState === WebSocket.OPEN) {
                // è½¬æ¢ä¸º base64 å‘é€
                const reader = new FileReader();
                reader.onloadend = () => {
                    const base64 = reader.result.split(',')[1];
                    this.ws.send(JSON.stringify({
                        type: 'input_audio_buffer.append',
                        audio: base64
                    }));
                };
                reader.readAsDataURL(event.data);
            }
        };
        
        // æ¯ 100ms å‘é€ä¸€æ¬¡æ•°æ®
        this.mediaRecorder.start(100);
    }
    
    // åœæ­¢è¯†åˆ«
    stop() {
        if (this.mediaRecorder) {
            this.mediaRecorder.stop();
            this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
        }
        
        if (this.ws) {
            this.ws.close();
        }
    }
    
    // å¤„ç†æœåŠ¡å™¨æ¶ˆæ¯
    handleMessage(data) {
        switch (data.type) {
            case 'response.audio_transcript.delta':
                // å®æ—¶è½¬å½•ç»“æœ
                if (this.onTranscript) {
                    this.onTranscript(data.delta.text);
                }
                break;
                
            case 'response.audio_transcript.done':
                // å¥å­ç»“æŸ
                if (this.onSentenceEnd) {
                    this.onSentenceEnd();
                }
                break;
                
            case 'error':
                // é”™è¯¯å¤„ç†
                if (this.onError) {
                    this.onError(data.error);
                }
                break;
        }
    }
}

// ä½¿ç”¨ç¤ºä¾‹
const realtimeASR = new RealtimeVoiceRecognition();

// è®¾ç½®å›è°ƒ
realtimeASR.onTranscript = (text) => {
    document.getElementById('realtime-text').textContent += text;
};

realtimeASR.onSentenceEnd = () => {
    document.getElementById('realtime-text').textContent += '\n';
};

realtimeASR.onError = (error) => {
    console.error('å®æ—¶è¯†åˆ«é”™è¯¯:', error);
};

// å¼€å§‹è¯†åˆ«
document.getElementById('start-realtime').onclick = async () => {
    await realtimeASR.startStreaming();
};

// åœæ­¢è¯†åˆ«
document.getElementById('stop-realtime').onclick = () => {
    realtimeASR.stop();
};
```

### 2. ä½¿ç”¨ Web Audio API çš„é«˜çº§å®ç°

```javascript
// advanced-realtime.js
class AdvancedRealtimeASR {
    constructor(options = {}) {
        this.options = {
            wsURL: 'ws://localhost:8000/v1/realtime',
            model: 'Systran/faster-distil-whisper-large-v3',
            language: 'zh',
            bufferSize: 4096,
            ...options
        };
        
        this.audioContext = null;
        this.processor = null;
        this.ws = null;
    }
    
    async start() {
        // 1. å»ºç«‹ WebSocket è¿æ¥
        await this.connectWebSocket();
        
        // 2. è·å–éŸ³é¢‘æµ
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        
        // 3. è®¾ç½® Web Audio API
        this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 16000
        });
        
        const source = this.audioContext.createMediaStreamSource(stream);
        
        // åˆ›å»º ScriptProcessor (å·²åºŸå¼ƒä½†ä»å¹¿æ³›æ”¯æŒ)
        this.processor = this.audioContext.createScriptProcessor(
            this.options.bufferSize, 
            1, 
            1
        );
        
        // å¤„ç†éŸ³é¢‘æ•°æ®
        this.processor.onaudioprocess = (e) => {
            if (this.ws.readyState === WebSocket.OPEN) {
                const inputData = e.inputBuffer.getChannelData(0);
                
                // è½¬æ¢ä¸º 16 ä½ PCM
                const pcm16 = this.float32ToPCM16(inputData);
                
                // è½¬æ¢ä¸º base64
                const base64 = btoa(String.fromCharCode(...new Uint8Array(pcm16.buffer)));
                
                // å‘é€åˆ°æœåŠ¡å™¨
                this.ws.send(JSON.stringify({
                    type: 'input_audio_buffer.append',
                    audio: base64
                }));
            }
        };
        
        // è¿æ¥éŸ³é¢‘èŠ‚ç‚¹
        source.connect(this.processor);
        this.processor.connect(this.audioContext.destination);
    }
    
    connectWebSocket() {
        return new Promise((resolve, reject) => {
            this.ws = new WebSocket(this.options.wsURL);
            
            this.ws.onopen = () => {
                // é…ç½®ä¼šè¯
                this.ws.send(JSON.stringify({
                    type: 'session.update',
                    session: {
                        model: this.options.model,
                        transcription_options: {
                            language: this.options.language
                        }
                    }
                }));
                resolve();
            };
            
            this.ws.onmessage = (event) => {
                const data = JSON.parse(event.data);
                
                if (data.type === 'response.audio_transcript.delta') {
                    this.onPartialTranscript?.(data.delta.text);
                } else if (data.type === 'response.audio_transcript.done') {
                    this.onFinalTranscript?.(data.transcript);
                }
            };
            
            this.ws.onerror = reject;
        });
    }
    
    // Float32 è½¬ 16ä½ PCM
    float32ToPCM16(float32Array) {
        const pcm16 = new Int16Array(float32Array.length);
        for (let i = 0; i < float32Array.length; i++) {
            const s = Math.max(-1, Math.min(1, float32Array[i]));
            pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }
        return pcm16;
    }
    
    stop() {
        if (this.processor) {
            this.processor.disconnect();
        }
        if (this.audioContext) {
            this.audioContext.close();
        }
        if (this.ws) {
            this.ws.close();
        }
    }
}
```

## å®Œæ•´ç¤ºä¾‹

### 1. è¯­éŸ³è¾“å…¥è¡¨å•

```html
<!DOCTYPE html>
<html>
<head>
    <title>è¯­éŸ³è¾“å…¥è¡¨å•</title>
    <style>
        .form-container {
            max-width: 600px;
            margin: 50px auto;
            padding: 20px;
        }
        
        .input-group {
            margin-bottom: 20px;
        }
        
        .input-with-voice {
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        input[type="text"] {
            flex: 1;
            padding: 10px;
            font-size: 16px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        
        .voice-btn {
            width: 40px;
            height: 40px;
            border: none;
            border-radius: 50%;
            background: #4CAF50;
            color: white;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        .voice-btn:hover {
            background: #45a049;
        }
        
        .voice-btn.recording {
            background: #f44336;
            animation: pulse 1s infinite;
        }
        
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.1); }
            100% { transform: scale(1); }
        }
    </style>
</head>
<body>
    <div class="form-container">
        <h2>è¯­éŸ³è¾“å…¥è¡¨å•ç¤ºä¾‹</h2>
        
        <div class="input-group">
            <label>å§“å:</label>
            <div class="input-with-voice">
                <input type="text" id="name" placeholder="è¯·è¾“å…¥æˆ–è¯´å‡ºæ‚¨çš„å§“å">
                <button class="voice-btn" data-target="name">ğŸ¤</button>
            </div>
        </div>
        
        <div class="input-group">
            <label>åœ°å€:</label>
            <div class="input-with-voice">
                <input type="text" id="address" placeholder="è¯·è¾“å…¥æˆ–è¯´å‡ºæ‚¨çš„åœ°å€">
                <button class="voice-btn" data-target="address">ğŸ¤</button>
            </div>
        </div>
        
        <div class="input-group">
            <label>ç•™è¨€:</label>
            <div class="input-with-voice">
                <input type="text" id="message" placeholder="è¯·è¾“å…¥æˆ–è¯´å‡ºæ‚¨çš„ç•™è¨€">
                <button class="voice-btn" data-target="message">ğŸ¤</button>
            </div>
        </div>
    </div>

    <script>
    class VoiceFormInput {
        constructor() {
            this.currentTarget = null;
            this.isRecording = false;
            this.mediaRecorder = null;
            this.audioChunks = [];
            
            this.config = {
                baseURL: 'http://localhost:8000',
                model: 'Systran/faster-distil-whisper-large-v3',
                language: 'zh'
            };
            
            this.init();
        }
        
        init() {
            // ä¸ºæ‰€æœ‰è¯­éŸ³æŒ‰é’®æ·»åŠ äº‹ä»¶
            document.querySelectorAll('.voice-btn').forEach(btn => {
                btn.addEventListener('mousedown', (e) => this.startRecording(e));
                btn.addEventListener('mouseup', () => this.stopRecording());
                btn.addEventListener('mouseleave', () => {
                    if (this.isRecording) this.stopRecording();
                });
                
                // ç§»åŠ¨ç«¯æ”¯æŒ
                btn.addEventListener('touchstart', (e) => {
                    e.preventDefault();
                    this.startRecording(e);
                });
                btn.addEventListener('touchend', (e) => {
                    e.preventDefault();
                    this.stopRecording();
                });
            });
        }
        
        async startRecording(event) {
            const btn = event.currentTarget;
            this.currentTarget = btn.dataset.target;
            
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                this.audioChunks = [];
                this.mediaRecorder = new MediaRecorder(stream);
                
                this.mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        this.audioChunks.push(event.data);
                    }
                };
                
                this.mediaRecorder.onstop = () => {
                    this.transcribeAudio();
                };
                
                this.mediaRecorder.start();
                this.isRecording = true;
                
                btn.classList.add('recording');
                
            } catch (err) {
                console.error('å½•éŸ³å¤±è´¥:', err);
                alert('æ— æ³•è®¿é—®éº¦å…‹é£');
            }
        }
        
        stopRecording() {
            if (this.mediaRecorder && this.isRecording) {
                this.mediaRecorder.stop();
                this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
                this.isRecording = false;
                
                // ç§»é™¤æ‰€æœ‰å½•éŸ³çŠ¶æ€
                document.querySelectorAll('.voice-btn').forEach(btn => {
                    btn.classList.remove('recording');
                });
            }
        }
        
        async transcribeAudio() {
            const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
            
            const formData = new FormData();
            formData.append('file', audioBlob, 'recording.webm');
            formData.append('model', this.config.model);
            formData.append('language', this.config.language);
            
            try {
                // æ˜¾ç¤ºå¤„ç†ä¸­çŠ¶æ€
                const input = document.getElementById(this.currentTarget);
                const originalValue = input.value;
                input.value = 'è¯†åˆ«ä¸­...';
                input.disabled = true;
                
                const response = await fetch(`${this.config.baseURL}/v1/audio/transcriptions`, {
                    method: 'POST',
                    body: formData
                });
                
                if (response.ok) {
                    const result = await response.json();
                    
                    // è¿½åŠ åˆ°è¾“å…¥æ¡†ï¼ˆè€Œä¸æ˜¯æ›¿æ¢ï¼‰
                    if (originalValue && originalValue !== 'è¯†åˆ«ä¸­...') {
                        input.value = originalValue + ' ' + result.text;
                    } else {
                        input.value = result.text;
                    }
                } else {
                    throw new Error('è¯†åˆ«å¤±è´¥');
                }
                
            } catch (err) {
                console.error('è½¬å½•å¤±è´¥:', err);
                document.getElementById(this.currentTarget).value = '';
                alert('è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•');
            } finally {
                const input = document.getElementById(this.currentTarget);
                input.disabled = false;
                input.focus();
            }
        }
    }
    
    // åˆå§‹åŒ–
    new VoiceFormInput();
    </script>
</body>
</html>
```

### 2. èŠå¤©åº”ç”¨é›†æˆ

```javascript
// chat-with-voice.js
class ChatWithVoice {
    constructor(chatContainerId) {
        this.chatContainer = document.getElementById(chatContainerId);
        this.voiceInput = new AdvancedVoiceInput({
            language: 'zh',
            maxDuration: 30000
        });
        
        this.setupVoiceInput();
    }
    
    setupVoiceInput() {
        // è¯­éŸ³è¾“å…¥æŒ‰é’®
        const voiceBtn = document.createElement('button');
        voiceBtn.className = 'chat-voice-btn';
        voiceBtn.innerHTML = 'ğŸ¤';
        
        // æŒ‰ä½å½•éŸ³
        voiceBtn.addEventListener('mousedown', () => {
            this.voiceInput.startRecording();
            voiceBtn.classList.add('recording');
        });
        
        voiceBtn.addEventListener('mouseup', () => {
            this.voiceInput.stopRecording();
            voiceBtn.classList.remove('recording');
        });
        
        // è®¾ç½®å›è°ƒ
        this.voiceInput.on('onResult', (result) => {
            this.sendMessage(result.text);
        });
        
        // æ·»åŠ åˆ°èŠå¤©ç•Œé¢
        this.chatContainer.appendChild(voiceBtn);
    }
    
    sendMessage(text) {
        // åˆ›å»ºæ¶ˆæ¯å…ƒç´ 
        const message = document.createElement('div');
        message.className = 'chat-message user';
        message.textContent = text;
        
        this.chatContainer.appendChild(message);
        
        // å‘é€åˆ°æœåŠ¡å™¨
        this.sendToServer(text);
    }
    
    async sendToServer(text) {
        // å®ç°å‘é€é€»è¾‘
        try {
            const response = await fetch('/api/chat', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ message: text })
            });
            
            const reply = await response.json();
            this.displayReply(reply.message);
            
        } catch (err) {
            console.error('å‘é€å¤±è´¥:', err);
        }
    }
    
    displayReply(text) {
        const message = document.createElement('div');
        message.className = 'chat-message bot';
        message.textContent = text;
        
        this.chatContainer.appendChild(message);
    }
}
```

## é”™è¯¯å¤„ç†

### 1. å¸¸è§é”™è¯¯å¤„ç†

```javascript
class VoiceInputWithErrorHandling {
    constructor() {
        this.errors = {
            NO_MICROPHONE: 'æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®',
            NETWORK_ERROR: 'ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ',
            SERVER_ERROR: 'æœåŠ¡å™¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•',
            TIMEOUT: 'è¯†åˆ«è¶…æ—¶ï¼Œè¯·é‡è¯•',
            NO_SPEECH: 'æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡æ–°å½•éŸ³',
            FORMAT_ERROR: 'éŸ³é¢‘æ ¼å¼ä¸æ”¯æŒ'
        };
    }
    
    async startRecording() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            // ... å½•éŸ³é€»è¾‘
        } catch (err) {
            this.handleError('NO_MICROPHONE', err);
        }
    }
    
    async transcribe(audioBlob) {
        try {
            const response = await this.fetchWithTimeout(
                `${this.config.baseURL}/v1/audio/transcriptions`,
                {
                    method: 'POST',
                    body: this.createFormData(audioBlob)
                },
                30000  // 30ç§’è¶…æ—¶
            );
            
            if (!response.ok) {
                if (response.status >= 500) {
                    throw new Error('SERVER_ERROR');
                } else if (response.status === 400) {
                    throw new Error('FORMAT_ERROR');
                }
            }
            
            const result = await response.json();
            
            if (!result.text || result.text.trim() === '') {
                throw new Error('NO_SPEECH');
            }
            
            return result;
            
        } catch (err) {
            if (err.name === 'AbortError') {
                this.handleError('TIMEOUT', err);
            } else if (err.message in this.errors) {
                this.handleError(err.message, err);
            } else {
                this.handleError('NETWORK_ERROR', err);
            }
        }
    }
    
    fetchWithTimeout(url, options, timeout) {
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), timeout);
        
        return fetch(url, {
            ...options,
            signal: controller.signal
        }).finally(() => {
            clearTimeout(timeoutId);
        });
    }
    
    handleError(errorType, error) {
        console.error(`Voice input error [${errorType}]:`, error);
        
        const errorMessage = this.errors[errorType] || 'æœªçŸ¥é”™è¯¯';
        
        // æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        this.showError(errorMessage);
        
        // è§¦å‘é”™è¯¯å›è°ƒ
        this.onError?.(errorType, errorMessage, error);
    }
    
    showError(message) {
        // æ˜¾ç¤ºé”™è¯¯æç¤º
        const errorEl = document.createElement('div');
        errorEl.className = 'voice-error';
        errorEl.textContent = message;
        
        document.body.appendChild(errorEl);
        
        setTimeout(() => {
            errorEl.remove();
        }, 3000);
    }
}
```

### 2. é‡è¯•æœºåˆ¶

```javascript
class VoiceInputWithRetry {
    constructor() {
        this.maxRetries = 3;
        this.retryDelay = 1000;
    }
    
    async transcribeWithRetry(audioBlob, retryCount = 0) {
        try {
            return await this.transcribe(audioBlob);
            
        } catch (err) {
            if (retryCount < this.maxRetries) {
                console.log(`é‡è¯• ${retryCount + 1}/${this.maxRetries}...`);
                
                await this.delay(this.retryDelay * (retryCount + 1));
                
                return this.transcribeWithRetry(audioBlob, retryCount + 1);
            } else {
                throw err;
            }
        }
    }
    
    delay(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }
}
```

## æœ€ä½³å®è·µ

### 1. æ€§èƒ½ä¼˜åŒ–

```javascript
// éŸ³é¢‘å‹ç¼©å’Œä¼˜åŒ–
class OptimizedVoiceInput {
    constructor() {
        this.audioContext = new AudioContext();
    }
    
    // é™é‡‡æ ·åˆ° 16kHz
    async downsampleAudio(audioBlob) {
        const arrayBuffer = await audioBlob.arrayBuffer();
        const audioBuffer = await this.audioContext.decodeAudioData(arrayBuffer);
        
        // åˆ›å»ºç¦»çº¿éŸ³é¢‘ä¸Šä¸‹æ–‡è¿›è¡Œé‡é‡‡æ ·
        const offlineContext = new OfflineAudioContext(
            1,  // å•å£°é“
            audioBuffer.duration * 16000,  // 16kHz
            16000
        );
        
        const source = offlineContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(offlineContext.destination);
        source.start();
        
        const resampled = await offlineContext.startRendering();
        
        // è½¬æ¢ä¸º WAV
        return this.audioBufferToWav(resampled);
    }
    
    // AudioBuffer è½¬ WAV
    audioBufferToWav(buffer) {
        const length = buffer.length * buffer.numberOfChannels * 2 + 44;
        const arrayBuffer = new ArrayBuffer(length);
        const view = new DataView(arrayBuffer);
        const channels = [];
        let offset = 0;
        let pos = 0;
        
        // WAV å¤´éƒ¨
        setUint32(0x46464952);  // "RIFF"
        setUint32(length - 8);  // file length - 8
        setUint32(0x45564157);  // "WAVE"
        
        setUint32(0x20746d66);  // "fmt " chunk
        setUint32(16);  // length = 16
        setUint16(1);  // PCM
        setUint16(buffer.numberOfChannels);
        setUint32(buffer.sampleRate);
        setUint32(buffer.sampleRate * 2 * buffer.numberOfChannels);  // avg. bytes/sec
        setUint16(buffer.numberOfChannels * 2);  // block-align
        setUint16(16);  // 16-bit
        
        setUint32(0x61746164);  // "data" chunk
        setUint32(length - pos - 4);  // chunk length
        
        // å†™å…¥ PCM æ•°æ®
        for (let i = 0; i < buffer.numberOfChannels; i++) {
            channels.push(buffer.getChannelData(i));
        }
        
        while (pos < length) {
            for (let i = 0; i < buffer.numberOfChannels; i++) {
                let sample = Math.max(-1, Math.min(1, channels[i][offset]));
                sample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
                view.setInt16(pos, sample, true);
                pos += 2;
            }
            offset++;
        }
        
        return new Blob([arrayBuffer], { type: 'audio/wav' });
        
        function setUint16(data) {
            view.setUint16(pos, data, true);
            pos += 2;
        }
        
        function setUint32(data) {
            view.setUint32(pos, data, true);
            pos += 4;
        }
    }
}
```

### 2. ç”¨æˆ·ä½“éªŒä¼˜åŒ–

```javascript
// å¸¦æœ‰ VADï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰çš„å®ç°
class VoiceInputWithVAD {
    constructor() {
        this.audioContext = new AudioContext();
        this.analyser = null;
        this.silenceTimeout = null;
        this.silenceDuration = 2000;  // 2ç§’é™éŸ³åè‡ªåŠ¨åœæ­¢
    }
    
    setupVAD(stream) {
        const source = this.audioContext.createMediaStreamSource(stream);
        this.analyser = this.audioContext.createAnalyser();
        this.analyser.fftSize = 256;
        
        source.connect(this.analyser);
        
        this.checkAudioLevel();
    }
    
    checkAudioLevel() {
        const dataArray = new Uint8Array(this.analyser.frequencyBinCount);
        this.analyser.getByteFrequencyData(dataArray);
        
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        
        if (average > 10) {  // æœ‰å£°éŸ³
            this.clearSilenceTimeout();
            this.onSpeechDetected?.();
        } else {  // é™éŸ³
            if (!this.silenceTimeout) {
                this.silenceTimeout = setTimeout(() => {
                    this.onSilenceDetected?.();
                    this.stopRecording();
                }, this.silenceDuration);
            }
        }
        
        if (this.isRecording) {
            requestAnimationFrame(() => this.checkAudioLevel());
        }
    }
    
    clearSilenceTimeout() {
        if (this.silenceTimeout) {
            clearTimeout(this.silenceTimeout);
            this.silenceTimeout = null;
        }
    }
}
```

### 3. å®‰å…¨æ€§è€ƒè™‘

```javascript
// å®‰å…¨çš„è¯­éŸ³è¾“å…¥å®ç°
class SecureVoiceInput {
    constructor() {
        this.config = {
            baseURL: this.getSecureBaseURL(),
            maxFileSize: 10 * 1024 * 1024,  // 10MB
            allowedFormats: ['audio/webm', 'audio/wav', 'audio/mp3'],
            csrfToken: this.getCSRFToken()
        };
    }
    
    getSecureBaseURL() {
        // ä½¿ç”¨ HTTPS
        const url = process.env.SPEACHES_URL || 'http://localhost:8000';
        return url.replace('http://', 'https://');
    }
    
    getCSRFToken() {
        // ä» cookie æˆ– meta æ ‡ç­¾è·å– CSRF token
        return document.querySelector('meta[name="csrf-token"]')?.content || '';
    }
    
    async transcribe(audioBlob) {
        // éªŒè¯æ–‡ä»¶å¤§å°
        if (audioBlob.size > this.config.maxFileSize) {
            throw new Error('éŸ³é¢‘æ–‡ä»¶è¿‡å¤§');
        }
        
        // éªŒè¯æ–‡ä»¶ç±»å‹
        if (!this.config.allowedFormats.includes(audioBlob.type)) {
            throw new Error('ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼');
        }
        
        const formData = new FormData();
        formData.append('file', audioBlob);
        formData.append('model', this.config.model);
        
        const response = await fetch(`${this.config.baseURL}/v1/audio/transcriptions`, {
            method: 'POST',
            headers: {
                'X-CSRF-Token': this.config.csrfToken
            },
            body: formData,
            credentials: 'same-origin'  // åŒ…å« cookies
        });
        
        return response.json();
    }
}
```

## éƒ¨ç½²æ³¨æ„äº‹é¡¹

### 1. HTTPS è¦æ±‚
- æµè§ˆå™¨è¦æ±‚ HTTPS æ‰èƒ½è®¿é—®éº¦å…‹é£
- æœ¬åœ°å¼€å‘å¯ä»¥ä½¿ç”¨ localhost

### 2. CORS é…ç½®
```javascript
// å¦‚æœå‰ç«¯å’Œåç«¯ä¸åŒåŸŸ
const response = await fetch('https://api.example.com/v1/audio/transcriptions', {
    method: 'POST',
    mode: 'cors',
    credentials: 'include',
    headers: {
        'Origin': window.location.origin
    },
    body: formData
});
```

### 3. ä»£ç†é…ç½®ï¼ˆå¼€å‘ç¯å¢ƒï¼‰
```javascript
// vite.config.js
export default {
  server: {
    proxy: {
      '/v1': {
        target: 'http://localhost:8000',
        changeOrigin: true
      }
    }
  }
}

// webpack.config.js
module.exports = {
  devServer: {
    proxy: {
      '/v1': 'http://localhost:8000'
    }
  }
}
```

## æµ‹è¯•å’Œè°ƒè¯•

### 1. æ¨¡æ‹ŸéŸ³é¢‘è¾“å…¥
```javascript
// ç”¨äºæµ‹è¯•çš„æ¨¡æ‹ŸéŸ³é¢‘
async function createTestAudio() {
    const audioContext = new AudioContext();
    const oscillator = audioContext.createOscillator();
    const destination = audioContext.createMediaStreamDestination();
    
    oscillator.connect(destination);
    oscillator.frequency.value = 440;  // A4 éŸ³ç¬¦
    oscillator.start();
    
    setTimeout(() => oscillator.stop(), 1000);  // 1ç§’
    
    return destination.stream;
}
```

### 2. è°ƒè¯•å·¥å…·
```javascript
// è¯­éŸ³è¾“å…¥è°ƒè¯•å™¨
class VoiceInputDebugger {
    constructor(voiceInput) {
        this.voiceInput = voiceInput;
        this.logs = [];
        
        this.interceptMethods();
        this.createDebugPanel();
    }
    
    interceptMethods() {
        const originalStart = this.voiceInput.startRecording;
        this.voiceInput.startRecording = async (...args) => {
            this.log('å¼€å§‹å½•éŸ³', { timestamp: Date.now() });
            return originalStart.apply(this.voiceInput, args);
        };
        
        const originalTranscribe = this.voiceInput.transcribe;
        this.voiceInput.transcribe = async (...args) => {
            const start = Date.now();
            try {
                const result = await originalTranscribe.apply(this.voiceInput, args);
                this.log('è½¬å½•æˆåŠŸ', { 
                    duration: Date.now() - start,
                    result: result.text 
                });
                return result;
            } catch (err) {
                this.log('è½¬å½•å¤±è´¥', { 
                    duration: Date.now() - start,
                    error: err.message 
                });
                throw err;
            }
        };
    }
    
    log(message, data) {
        const entry = {
            time: new Date().toLocaleTimeString(),
            message,
            data
        };
        
        this.logs.push(entry);
        this.updateDebugPanel(entry);
    }
    
    createDebugPanel() {
        const panel = document.createElement('div');
        panel.id = 'voice-debug-panel';
        panel.style.cssText = `
            position: fixed;
            bottom: 20px;
            right: 20px;
            width: 300px;
            max-height: 400px;
            background: rgba(0, 0, 0, 0.8);
            color: white;
            padding: 10px;
            border-radius: 8px;
            font-family: monospace;
            font-size: 12px;
            overflow-y: auto;
            z-index: 9999;
        `;
        
        document.body.appendChild(panel);
    }
    
    updateDebugPanel(entry) {
        const panel = document.getElementById('voice-debug-panel');
        const line = document.createElement('div');
        line.textContent = `[${entry.time}] ${entry.message}`;
        
        if (entry.data) {
            const details = document.createElement('pre');
            details.textContent = JSON.stringify(entry.data, null, 2);
            details.style.marginLeft = '10px';
            line.appendChild(details);
        }
        
        panel.appendChild(line);
        panel.scrollTop = panel.scrollHeight;
    }
}

// ä½¿ç”¨è°ƒè¯•å™¨
const voiceInput = new VoiceInput();
const debugger = new VoiceInputDebugger(voiceInput);
```

## æµè§ˆå™¨å…¼å®¹æ€§

### æ”¯æŒçš„æµè§ˆå™¨
- Chrome 25+
- Firefox 29+
- Safari 11+
- Edge 12+

### å…¼å®¹æ€§æ£€æŸ¥
```javascript
function checkBrowserSupport() {
    const support = {
        getUserMedia: !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia),
        mediaRecorder: typeof MediaRecorder !== 'undefined',
        webAudio: typeof AudioContext !== 'undefined' || typeof webkitAudioContext !== 'undefined',
        webSocket: typeof WebSocket !== 'undefined'
    };
    
    const isSupported = Object.values(support).every(v => v);
    
    if (!isSupported) {
        console.warn('æµè§ˆå™¨å…¼å®¹æ€§:', support);
    }
    
    return isSupported;
}
```

---

è¿™ä¸ªæ–‡æ¡£æä¾›äº†å®Œæ•´çš„å‰ç«¯æ¥å…¥æ–¹æ¡ˆï¼ŒåŒ…æ‹¬éå®æ—¶å’Œå®æ—¶ä¸¤ç§æ¨¡å¼ï¼Œä»¥åŠå„ç§æ¡†æ¶çš„å®ç°ç¤ºä¾‹ã€‚æ‚¨å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„å®ç°æ–¹å¼ã€‚