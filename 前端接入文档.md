# Speaches 语音转文字前端接入文档

## 目录
- [快速开始](#快速开始)
- [非实时语音转文字](#非实时语音转文字)
- [实时语音转文字](#实时语音转文字)
- [完整示例](#完整示例)
- [错误处理](#错误处理)
- [最佳实践](#最佳实践)

## 快速开始

### 服务地址
```javascript
const SPEACHES_BASE_URL = 'http://localhost:8000';  // 本地服务
// const SPEACHES_BASE_URL = 'https://your-api.com'; // 生产环境
```

### 推荐模型
```javascript
// ASR (语音识别) 模型
const ASR_MODEL = 'Systran/faster-distil-whisper-large-v3';  // 推荐：速度快，准确率高

// 语言设置
const LANGUAGE = 'zh';  // 中文
// const LANGUAGE = 'en';  // 英文
// const LANGUAGE = '';    // 自动检测
```

## 非实时语音转文字

### 1. 基础实现（类似微信语音输入）

```html
<!DOCTYPE html>
<html>
<head>
    <title>语音转文字</title>
</head>
<body>
    <button id="recordBtn">按住说话</button>
    <div id="result"></div>

    <script>
    class VoiceToText {
        constructor() {
            this.mediaRecorder = null;
            this.audioChunks = [];
            this.isRecording = false;
            
            // 配置
            this.config = {
                baseURL: 'http://localhost:8000',
                model: 'Systran/faster-distil-whisper-large-v3',
                language: 'zh'
            };
            
            this.init();
        }
        
        init() {
            const btn = document.getElementById('recordBtn');
            
            // 鼠标事件
            btn.addEventListener('mousedown', () => this.startRecording());
            btn.addEventListener('mouseup', () => this.stopRecording());
            btn.addEventListener('mouseleave', () => {
                if (this.isRecording) this.stopRecording();
            });
            
            // 触摸事件（移动端）
            btn.addEventListener('touchstart', (e) => {
                e.preventDefault();
                this.startRecording();
            });
            btn.addEventListener('touchend', (e) => {
                e.preventDefault();
                this.stopRecording();
            });
        }
        
        async startRecording() {
            try {
                // 获取麦克风权限
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        channelCount: 1,
                        sampleRate: 16000
                    } 
                });
                
                this.audioChunks = [];
                this.mediaRecorder = new MediaRecorder(stream);
                
                this.mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        this.audioChunks.push(event.data);
                    }
                };
                
                this.mediaRecorder.onstop = () => {
                    this.transcribeAudio();
                };
                
                this.mediaRecorder.start();
                this.isRecording = true;
                
                document.getElementById('recordBtn').textContent = '松开结束';
                
            } catch (err) {
                console.error('录音失败:', err);
                alert('无法访问麦克风');
            }
        }
        
        stopRecording() {
            if (this.mediaRecorder && this.isRecording) {
                this.mediaRecorder.stop();
                this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
                this.isRecording = false;
                
                document.getElementById('recordBtn').textContent = '按住说话';
            }
        }
        
        async transcribeAudio() {
            // 创建音频 Blob
            const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
            
            // 准备表单数据
            const formData = new FormData();
            formData.append('file', audioBlob, 'recording.webm');
            formData.append('model', this.config.model);
            formData.append('language', this.config.language);
            formData.append('response_format', 'json');
            
            try {
                document.getElementById('result').textContent = '识别中...';
                
                const response = await fetch(`${this.config.baseURL}/v1/audio/transcriptions`, {
                    method: 'POST',
                    body: formData
                });
                
                if (response.ok) {
                    const result = await response.json();
                    document.getElementById('result').textContent = result.text || '(无法识别)';
                } else {
                    throw new Error(`HTTP ${response.status}`);
                }
                
            } catch (err) {
                console.error('转录失败:', err);
                document.getElementById('result').textContent = '识别失败';
            }
        }
    }
    
    // 初始化
    new VoiceToText();
    </script>
</body>
</html>
```

### 2. 高级实现（带 UI 反馈）

```javascript
// voice-input.js
class AdvancedVoiceInput {
    constructor(options = {}) {
        this.options = {
            baseURL: 'http://localhost:8000',
            model: 'Systran/faster-distil-whisper-large-v3',
            language: 'zh',
            maxDuration: 60000,  // 最长录音时间 60 秒
            ...options
        };
        
        this.callbacks = {
            onStart: null,
            onStop: null,
            onResult: null,
            onError: null
        };
        
        this.audioContext = null;
        this.analyser = null;
    }
    
    // 设置回调
    on(event, callback) {
        if (this.callbacks.hasOwnProperty(event)) {
            this.callbacks[event] = callback;
        }
    }
    
    // 开始录音
    async startRecording() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    sampleRate: 16000
                } 
            });
            
            // 音频可视化
            this.setupAudioVisualization(stream);
            
            // 设置 MediaRecorder
            this.audioChunks = [];
            this.mediaRecorder = new MediaRecorder(stream, {
                mimeType: 'audio/webm;codecs=opus'
            });
            
            this.mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    this.audioChunks.push(event.data);
                }
            };
            
            this.mediaRecorder.onstop = () => {
                this.handleRecordingComplete();
            };
            
            // 开始录音
            this.mediaRecorder.start();
            this.startTime = Date.now();
            
            // 设置最大录音时长
            this.maxDurationTimeout = setTimeout(() => {
                this.stopRecording();
            }, this.options.maxDuration);
            
            // 触发回调
            this.callbacks.onStart?.();
            
        } catch (err) {
            this.callbacks.onError?.(err);
        }
    }
    
    // 停止录音
    stopRecording() {
        if (this.mediaRecorder && this.mediaRecorder.state === 'recording') {
            clearTimeout(this.maxDurationTimeout);
            
            this.mediaRecorder.stop();
            this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
            
            // 停止音频分析
            if (this.audioContext) {
                this.audioContext.close();
            }
            
            const duration = (Date.now() - this.startTime) / 1000;
            this.callbacks.onStop?.(duration);
        }
    }
    
    // 处理录音完成
    async handleRecordingComplete() {
        const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
        
        // 转换为 WAV（可选，提高兼容性）
        // const wavBlob = await this.convertToWav(audioBlob);
        
        // 发送到服务器
        await this.transcribe(audioBlob);
    }
    
    // 语音转文字
    async transcribe(audioBlob) {
        const formData = new FormData();
        formData.append('file', audioBlob, 'recording.webm');
        formData.append('model', this.options.model);
        formData.append('language', this.options.language);
        formData.append('response_format', 'verbose_json');  // 获取详细信息
        
        try {
            const response = await fetch(`${this.options.baseURL}/v1/audio/transcriptions`, {
                method: 'POST',
                body: formData
            });
            
            if (!response.ok) {
                throw new Error(`HTTP ${response.status}`);
            }
            
            const result = await response.json();
            
            // 处理结果
            this.callbacks.onResult?.({
                text: result.text,
                language: result.language,
                duration: result.duration,
                words: result.words  // 词级时间戳
            });
            
        } catch (err) {
            this.callbacks.onError?.(err);
        }
    }
    
    // 音频可视化
    setupAudioVisualization(stream) {
        this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const source = this.audioContext.createMediaStreamSource(stream);
        
        this.analyser = this.audioContext.createAnalyser();
        this.analyser.fftSize = 256;
        
        source.connect(this.analyser);
        
        // 返回分析器供 UI 使用
        return this.analyser;
    }
    
    // 获取音频电平
    getAudioLevel() {
        if (!this.analyser) return 0;
        
        const dataArray = new Uint8Array(this.analyser.frequencyBinCount);
        this.analyser.getByteFrequencyData(dataArray);
        
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        return average / 255;  // 归一化到 0-1
    }
}

// 使用示例
const voiceInput = new AdvancedVoiceInput({
    language: 'zh',
    maxDuration: 30000  // 30秒
});

// 设置回调
voiceInput.on('onStart', () => {
    console.log('开始录音');
    updateUI('recording');
});

voiceInput.on('onStop', (duration) => {
    console.log(`录音结束，时长: ${duration}秒`);
    updateUI('processing');
});

voiceInput.on('onResult', (result) => {
    console.log('识别结果:', result.text);
    displayResult(result);
});

voiceInput.on('onError', (err) => {
    console.error('错误:', err);
    showError(err.message);
});
```

### 3. React 组件实现

```jsx
// VoiceInput.jsx
import React, { useState, useRef } from 'react';

const VoiceInput = ({ onTranscription }) => {
    const [isRecording, setIsRecording] = useState(false);
    const [transcription, setTranscription] = useState('');
    const [isProcessing, setIsProcessing] = useState(false);
    const mediaRecorderRef = useRef(null);
    const audioChunksRef = useRef([]);
    
    const config = {
        baseURL: process.env.REACT_APP_SPEACHES_URL || 'http://localhost:8000',
        model: 'Systran/faster-distil-whisper-large-v3',
        language: 'zh'
    };
    
    const startRecording = async () => {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            
            audioChunksRef.current = [];
            mediaRecorderRef.current = new MediaRecorder(stream);
            
            mediaRecorderRef.current.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    audioChunksRef.current.push(event.data);
                }
            };
            
            mediaRecorderRef.current.onstop = handleRecordingComplete;
            
            mediaRecorderRef.current.start();
            setIsRecording(true);
            
        } catch (err) {
            console.error('Failed to start recording:', err);
            alert('无法访问麦克风');
        }
    };
    
    const stopRecording = () => {
        if (mediaRecorderRef.current && isRecording) {
            mediaRecorderRef.current.stop();
            mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
            setIsRecording(false);
        }
    };
    
    const handleRecordingComplete = async () => {
        setIsProcessing(true);
        
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        
        const formData = new FormData();
        formData.append('file', audioBlob, 'recording.webm');
        formData.append('model', config.model);
        formData.append('language', config.language);
        
        try {
            const response = await fetch(`${config.baseURL}/v1/audio/transcriptions`, {
                method: 'POST',
                body: formData
            });
            
            if (response.ok) {
                const result = await response.json();
                setTranscription(result.text);
                onTranscription?.(result.text);
            } else {
                throw new Error('Transcription failed');
            }
            
        } catch (err) {
            console.error('Transcription error:', err);
            setTranscription('识别失败');
        } finally {
            setIsProcessing(false);
        }
    };
    
    return (
        <div className="voice-input">
            <button
                onMouseDown={startRecording}
                onMouseUp={stopRecording}
                onMouseLeave={stopRecording}
                onTouchStart={startRecording}
                onTouchEnd={stopRecording}
                disabled={isProcessing}
                className={`voice-button ${isRecording ? 'recording' : ''}`}
            >
                {isRecording ? '松开结束' : '按住说话'}
            </button>
            
            {isProcessing && <div className="processing">识别中...</div>}
            
            {transcription && (
                <div className="transcription">
                    {transcription}
                </div>
            )}
        </div>
    );
};

export default VoiceInput;
```

### 4. Vue 3 组件实现

```vue
<!-- VoiceInput.vue -->
<template>
  <div class="voice-input">
    <button
      @mousedown="startRecording"
      @mouseup="stopRecording"
      @mouseleave="handleMouseLeave"
      @touchstart.prevent="startRecording"
      @touchend.prevent="stopRecording"
      :disabled="isProcessing"
      :class="['voice-button', { recording: isRecording }]"
    >
      {{ buttonText }}
    </button>
    
    <div v-if="isProcessing" class="processing">识别中...</div>
    
    <div v-if="transcription" class="transcription">
      {{ transcription }}
    </div>
  </div>
</template>

<script setup>
import { ref, computed } from 'vue';

const emit = defineEmits(['transcription']);

const isRecording = ref(false);
const isProcessing = ref(false);
const transcription = ref('');
const mediaRecorder = ref(null);
const audioChunks = ref([]);

const config = {
  baseURL: import.meta.env.VITE_SPEACHES_URL || 'http://localhost:8000',
  model: 'Systran/faster-distil-whisper-large-v3',
  language: 'zh'
};

const buttonText = computed(() => {
  if (isRecording.value) return '松开结束';
  if (isProcessing.value) return '处理中...';
  return '按住说话';
});

const startRecording = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    
    audioChunks.value = [];
    mediaRecorder.value = new MediaRecorder(stream);
    
    mediaRecorder.value.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunks.value.push(event.data);
      }
    };
    
    mediaRecorder.value.onstop = handleRecordingComplete;
    
    mediaRecorder.value.start();
    isRecording.value = true;
    
  } catch (err) {
    console.error('录音失败:', err);
    alert('无法访问麦克风');
  }
};

const stopRecording = () => {
  if (mediaRecorder.value && isRecording.value) {
    mediaRecorder.value.stop();
    mediaRecorder.value.stream.getTracks().forEach(track => track.stop());
    isRecording.value = false;
  }
};

const handleMouseLeave = () => {
  if (isRecording.value) {
    stopRecording();
  }
};

const handleRecordingComplete = async () => {
  isProcessing.value = true;
  
  const audioBlob = new Blob(audioChunks.value, { type: 'audio/webm' });
  
  const formData = new FormData();
  formData.append('file', audioBlob, 'recording.webm');
  formData.append('model', config.model);
  formData.append('language', config.language);
  
  try {
    const response = await fetch(`${config.baseURL}/v1/audio/transcriptions`, {
      method: 'POST',
      body: formData
    });
    
    if (response.ok) {
      const result = await response.json();
      transcription.value = result.text;
      emit('transcription', result.text);
    } else {
      throw new Error('转录失败');
    }
    
  } catch (err) {
    console.error('转录错误:', err);
    transcription.value = '识别失败';
  } finally {
    isProcessing.value = false;
  }
};
</script>

<style scoped>
.voice-button {
  width: 120px;
  height: 120px;
  border-radius: 50%;
  background: #4CAF50;
  color: white;
  border: none;
  font-size: 16px;
  cursor: pointer;
  transition: all 0.3s;
}

.voice-button:hover {
  background: #45a049;
  transform: scale(1.05);
}

.voice-button.recording {
  background: #f44336;
  animation: pulse 1.5s infinite;
}

.voice-button:disabled {
  background: #ccc;
  cursor: not-allowed;
}

@keyframes pulse {
  0% {
    box-shadow: 0 0 0 0 rgba(244, 67, 54, 0.7);
  }
  70% {
    box-shadow: 0 0 0 20px rgba(244, 67, 54, 0);
  }
  100% {
    box-shadow: 0 0 0 0 rgba(244, 67, 54, 0);
  }
}

.processing {
  margin-top: 20px;
  color: #666;
}

.transcription {
  margin-top: 20px;
  padding: 15px;
  background: #f5f5f5;
  border-radius: 8px;
  min-height: 50px;
}
</style>
```

## 实时语音转文字

### 1. WebSocket 实时识别

```javascript
// realtime-voice.js
class RealtimeVoiceRecognition {
    constructor(options = {}) {
        this.options = {
            wsURL: 'ws://localhost:8000/v1/realtime',
            model: 'Systran/faster-distil-whisper-large-v3',
            language: 'zh',
            ...options
        };
        
        this.ws = null;
        this.mediaRecorder = null;
        this.isConnected = false;
    }
    
    // 连接 WebSocket
    async connect() {
        return new Promise((resolve, reject) => {
            this.ws = new WebSocket(this.options.wsURL);
            
            this.ws.onopen = () => {
                console.log('WebSocket 连接成功');
                this.isConnected = true;
                
                // 配置会话
                this.ws.send(JSON.stringify({
                    type: 'session.update',
                    session: {
                        model: this.options.model,
                        transcription_options: {
                            language: this.options.language,
                            temperature: 0.3,
                            beam_size: 5
                        }
                    }
                }));
                
                resolve();
            };
            
            this.ws.onmessage = (event) => {
                const data = JSON.parse(event.data);
                this.handleMessage(data);
            };
            
            this.ws.onerror = (error) => {
                console.error('WebSocket 错误:', error);
                reject(error);
            };
            
            this.ws.onclose = () => {
                console.log('WebSocket 连接关闭');
                this.isConnected = false;
            };
        });
    }
    
    // 开始实时识别
    async startStreaming() {
        if (!this.isConnected) {
            await this.connect();
        }
        
        const stream = await navigator.mediaDevices.getUserMedia({ 
            audio: {
                channelCount: 1,
                sampleRate: 16000,
                echoCancellation: true,
                noiseSuppression: true
            } 
        });
        
        // 使用 MediaRecorder 捕获音频
        this.mediaRecorder = new MediaRecorder(stream, {
            mimeType: 'audio/webm;codecs=opus'
        });
        
        this.mediaRecorder.ondataavailable = async (event) => {
            if (event.data.size > 0 && this.ws.readyState === WebSocket.OPEN) {
                // 转换为 base64 发送
                const reader = new FileReader();
                reader.onloadend = () => {
                    const base64 = reader.result.split(',')[1];
                    this.ws.send(JSON.stringify({
                        type: 'input_audio_buffer.append',
                        audio: base64
                    }));
                };
                reader.readAsDataURL(event.data);
            }
        };
        
        // 每 100ms 发送一次数据
        this.mediaRecorder.start(100);
    }
    
    // 停止识别
    stop() {
        if (this.mediaRecorder) {
            this.mediaRecorder.stop();
            this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
        }
        
        if (this.ws) {
            this.ws.close();
        }
    }
    
    // 处理服务器消息
    handleMessage(data) {
        switch (data.type) {
            case 'response.audio_transcript.delta':
                // 实时转录结果
                if (this.onTranscript) {
                    this.onTranscript(data.delta.text);
                }
                break;
                
            case 'response.audio_transcript.done':
                // 句子结束
                if (this.onSentenceEnd) {
                    this.onSentenceEnd();
                }
                break;
                
            case 'error':
                // 错误处理
                if (this.onError) {
                    this.onError(data.error);
                }
                break;
        }
    }
}

// 使用示例
const realtimeASR = new RealtimeVoiceRecognition();

// 设置回调
realtimeASR.onTranscript = (text) => {
    document.getElementById('realtime-text').textContent += text;
};

realtimeASR.onSentenceEnd = () => {
    document.getElementById('realtime-text').textContent += '\n';
};

realtimeASR.onError = (error) => {
    console.error('实时识别错误:', error);
};

// 开始识别
document.getElementById('start-realtime').onclick = async () => {
    await realtimeASR.startStreaming();
};

// 停止识别
document.getElementById('stop-realtime').onclick = () => {
    realtimeASR.stop();
};
```

### 2. 使用 Web Audio API 的高级实现

```javascript
// advanced-realtime.js
class AdvancedRealtimeASR {
    constructor(options = {}) {
        this.options = {
            wsURL: 'ws://localhost:8000/v1/realtime',
            model: 'Systran/faster-distil-whisper-large-v3',
            language: 'zh',
            bufferSize: 4096,
            ...options
        };
        
        this.audioContext = null;
        this.processor = null;
        this.ws = null;
    }
    
    async start() {
        // 1. 建立 WebSocket 连接
        await this.connectWebSocket();
        
        // 2. 获取音频流
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        
        // 3. 设置 Web Audio API
        this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 16000
        });
        
        const source = this.audioContext.createMediaStreamSource(stream);
        
        // 创建 ScriptProcessor (已废弃但仍广泛支持)
        this.processor = this.audioContext.createScriptProcessor(
            this.options.bufferSize, 
            1, 
            1
        );
        
        // 处理音频数据
        this.processor.onaudioprocess = (e) => {
            if (this.ws.readyState === WebSocket.OPEN) {
                const inputData = e.inputBuffer.getChannelData(0);
                
                // 转换为 16 位 PCM
                const pcm16 = this.float32ToPCM16(inputData);
                
                // 转换为 base64
                const base64 = btoa(String.fromCharCode(...new Uint8Array(pcm16.buffer)));
                
                // 发送到服务器
                this.ws.send(JSON.stringify({
                    type: 'input_audio_buffer.append',
                    audio: base64
                }));
            }
        };
        
        // 连接音频节点
        source.connect(this.processor);
        this.processor.connect(this.audioContext.destination);
    }
    
    connectWebSocket() {
        return new Promise((resolve, reject) => {
            this.ws = new WebSocket(this.options.wsURL);
            
            this.ws.onopen = () => {
                // 配置会话
                this.ws.send(JSON.stringify({
                    type: 'session.update',
                    session: {
                        model: this.options.model,
                        transcription_options: {
                            language: this.options.language
                        }
                    }
                }));
                resolve();
            };
            
            this.ws.onmessage = (event) => {
                const data = JSON.parse(event.data);
                
                if (data.type === 'response.audio_transcript.delta') {
                    this.onPartialTranscript?.(data.delta.text);
                } else if (data.type === 'response.audio_transcript.done') {
                    this.onFinalTranscript?.(data.transcript);
                }
            };
            
            this.ws.onerror = reject;
        });
    }
    
    // Float32 转 16位 PCM
    float32ToPCM16(float32Array) {
        const pcm16 = new Int16Array(float32Array.length);
        for (let i = 0; i < float32Array.length; i++) {
            const s = Math.max(-1, Math.min(1, float32Array[i]));
            pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }
        return pcm16;
    }
    
    stop() {
        if (this.processor) {
            this.processor.disconnect();
        }
        if (this.audioContext) {
            this.audioContext.close();
        }
        if (this.ws) {
            this.ws.close();
        }
    }
}
```

## 完整示例

### 1. 语音输入表单

```html
<!DOCTYPE html>
<html>
<head>
    <title>语音输入表单</title>
    <style>
        .form-container {
            max-width: 600px;
            margin: 50px auto;
            padding: 20px;
        }
        
        .input-group {
            margin-bottom: 20px;
        }
        
        .input-with-voice {
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        input[type="text"] {
            flex: 1;
            padding: 10px;
            font-size: 16px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        
        .voice-btn {
            width: 40px;
            height: 40px;
            border: none;
            border-radius: 50%;
            background: #4CAF50;
            color: white;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        .voice-btn:hover {
            background: #45a049;
        }
        
        .voice-btn.recording {
            background: #f44336;
            animation: pulse 1s infinite;
        }
        
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.1); }
            100% { transform: scale(1); }
        }
    </style>
</head>
<body>
    <div class="form-container">
        <h2>语音输入表单示例</h2>
        
        <div class="input-group">
            <label>姓名:</label>
            <div class="input-with-voice">
                <input type="text" id="name" placeholder="请输入或说出您的姓名">
                <button class="voice-btn" data-target="name">🎤</button>
            </div>
        </div>
        
        <div class="input-group">
            <label>地址:</label>
            <div class="input-with-voice">
                <input type="text" id="address" placeholder="请输入或说出您的地址">
                <button class="voice-btn" data-target="address">🎤</button>
            </div>
        </div>
        
        <div class="input-group">
            <label>留言:</label>
            <div class="input-with-voice">
                <input type="text" id="message" placeholder="请输入或说出您的留言">
                <button class="voice-btn" data-target="message">🎤</button>
            </div>
        </div>
    </div>

    <script>
    class VoiceFormInput {
        constructor() {
            this.currentTarget = null;
            this.isRecording = false;
            this.mediaRecorder = null;
            this.audioChunks = [];
            
            this.config = {
                baseURL: 'http://localhost:8000',
                model: 'Systran/faster-distil-whisper-large-v3',
                language: 'zh'
            };
            
            this.init();
        }
        
        init() {
            // 为所有语音按钮添加事件
            document.querySelectorAll('.voice-btn').forEach(btn => {
                btn.addEventListener('mousedown', (e) => this.startRecording(e));
                btn.addEventListener('mouseup', () => this.stopRecording());
                btn.addEventListener('mouseleave', () => {
                    if (this.isRecording) this.stopRecording();
                });
                
                // 移动端支持
                btn.addEventListener('touchstart', (e) => {
                    e.preventDefault();
                    this.startRecording(e);
                });
                btn.addEventListener('touchend', (e) => {
                    e.preventDefault();
                    this.stopRecording();
                });
            });
        }
        
        async startRecording(event) {
            const btn = event.currentTarget;
            this.currentTarget = btn.dataset.target;
            
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                this.audioChunks = [];
                this.mediaRecorder = new MediaRecorder(stream);
                
                this.mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        this.audioChunks.push(event.data);
                    }
                };
                
                this.mediaRecorder.onstop = () => {
                    this.transcribeAudio();
                };
                
                this.mediaRecorder.start();
                this.isRecording = true;
                
                btn.classList.add('recording');
                
            } catch (err) {
                console.error('录音失败:', err);
                alert('无法访问麦克风');
            }
        }
        
        stopRecording() {
            if (this.mediaRecorder && this.isRecording) {
                this.mediaRecorder.stop();
                this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
                this.isRecording = false;
                
                // 移除所有录音状态
                document.querySelectorAll('.voice-btn').forEach(btn => {
                    btn.classList.remove('recording');
                });
            }
        }
        
        async transcribeAudio() {
            const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
            
            const formData = new FormData();
            formData.append('file', audioBlob, 'recording.webm');
            formData.append('model', this.config.model);
            formData.append('language', this.config.language);
            
            try {
                // 显示处理中状态
                const input = document.getElementById(this.currentTarget);
                const originalValue = input.value;
                input.value = '识别中...';
                input.disabled = true;
                
                const response = await fetch(`${this.config.baseURL}/v1/audio/transcriptions`, {
                    method: 'POST',
                    body: formData
                });
                
                if (response.ok) {
                    const result = await response.json();
                    
                    // 追加到输入框（而不是替换）
                    if (originalValue && originalValue !== '识别中...') {
                        input.value = originalValue + ' ' + result.text;
                    } else {
                        input.value = result.text;
                    }
                } else {
                    throw new Error('识别失败');
                }
                
            } catch (err) {
                console.error('转录失败:', err);
                document.getElementById(this.currentTarget).value = '';
                alert('识别失败，请重试');
            } finally {
                const input = document.getElementById(this.currentTarget);
                input.disabled = false;
                input.focus();
            }
        }
    }
    
    // 初始化
    new VoiceFormInput();
    </script>
</body>
</html>
```

### 2. 聊天应用集成

```javascript
// chat-with-voice.js
class ChatWithVoice {
    constructor(chatContainerId) {
        this.chatContainer = document.getElementById(chatContainerId);
        this.voiceInput = new AdvancedVoiceInput({
            language: 'zh',
            maxDuration: 30000
        });
        
        this.setupVoiceInput();
    }
    
    setupVoiceInput() {
        // 语音输入按钮
        const voiceBtn = document.createElement('button');
        voiceBtn.className = 'chat-voice-btn';
        voiceBtn.innerHTML = '🎤';
        
        // 按住录音
        voiceBtn.addEventListener('mousedown', () => {
            this.voiceInput.startRecording();
            voiceBtn.classList.add('recording');
        });
        
        voiceBtn.addEventListener('mouseup', () => {
            this.voiceInput.stopRecording();
            voiceBtn.classList.remove('recording');
        });
        
        // 设置回调
        this.voiceInput.on('onResult', (result) => {
            this.sendMessage(result.text);
        });
        
        // 添加到聊天界面
        this.chatContainer.appendChild(voiceBtn);
    }
    
    sendMessage(text) {
        // 创建消息元素
        const message = document.createElement('div');
        message.className = 'chat-message user';
        message.textContent = text;
        
        this.chatContainer.appendChild(message);
        
        // 发送到服务器
        this.sendToServer(text);
    }
    
    async sendToServer(text) {
        // 实现发送逻辑
        try {
            const response = await fetch('/api/chat', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ message: text })
            });
            
            const reply = await response.json();
            this.displayReply(reply.message);
            
        } catch (err) {
            console.error('发送失败:', err);
        }
    }
    
    displayReply(text) {
        const message = document.createElement('div');
        message.className = 'chat-message bot';
        message.textContent = text;
        
        this.chatContainer.appendChild(message);
    }
}
```

## 错误处理

### 1. 常见错误处理

```javascript
class VoiceInputWithErrorHandling {
    constructor() {
        this.errors = {
            NO_MICROPHONE: '无法访问麦克风，请检查权限设置',
            NETWORK_ERROR: '网络连接失败，请检查网络',
            SERVER_ERROR: '服务器错误，请稍后重试',
            TIMEOUT: '识别超时，请重试',
            NO_SPEECH: '未检测到语音，请重新录音',
            FORMAT_ERROR: '音频格式不支持'
        };
    }
    
    async startRecording() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            // ... 录音逻辑
        } catch (err) {
            this.handleError('NO_MICROPHONE', err);
        }
    }
    
    async transcribe(audioBlob) {
        try {
            const response = await this.fetchWithTimeout(
                `${this.config.baseURL}/v1/audio/transcriptions`,
                {
                    method: 'POST',
                    body: this.createFormData(audioBlob)
                },
                30000  // 30秒超时
            );
            
            if (!response.ok) {
                if (response.status >= 500) {
                    throw new Error('SERVER_ERROR');
                } else if (response.status === 400) {
                    throw new Error('FORMAT_ERROR');
                }
            }
            
            const result = await response.json();
            
            if (!result.text || result.text.trim() === '') {
                throw new Error('NO_SPEECH');
            }
            
            return result;
            
        } catch (err) {
            if (err.name === 'AbortError') {
                this.handleError('TIMEOUT', err);
            } else if (err.message in this.errors) {
                this.handleError(err.message, err);
            } else {
                this.handleError('NETWORK_ERROR', err);
            }
        }
    }
    
    fetchWithTimeout(url, options, timeout) {
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), timeout);
        
        return fetch(url, {
            ...options,
            signal: controller.signal
        }).finally(() => {
            clearTimeout(timeoutId);
        });
    }
    
    handleError(errorType, error) {
        console.error(`Voice input error [${errorType}]:`, error);
        
        const errorMessage = this.errors[errorType] || '未知错误';
        
        // 显示错误信息
        this.showError(errorMessage);
        
        // 触发错误回调
        this.onError?.(errorType, errorMessage, error);
    }
    
    showError(message) {
        // 显示错误提示
        const errorEl = document.createElement('div');
        errorEl.className = 'voice-error';
        errorEl.textContent = message;
        
        document.body.appendChild(errorEl);
        
        setTimeout(() => {
            errorEl.remove();
        }, 3000);
    }
}
```

### 2. 重试机制

```javascript
class VoiceInputWithRetry {
    constructor() {
        this.maxRetries = 3;
        this.retryDelay = 1000;
    }
    
    async transcribeWithRetry(audioBlob, retryCount = 0) {
        try {
            return await this.transcribe(audioBlob);
            
        } catch (err) {
            if (retryCount < this.maxRetries) {
                console.log(`重试 ${retryCount + 1}/${this.maxRetries}...`);
                
                await this.delay(this.retryDelay * (retryCount + 1));
                
                return this.transcribeWithRetry(audioBlob, retryCount + 1);
            } else {
                throw err;
            }
        }
    }
    
    delay(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }
}
```

## 最佳实践

### 1. 性能优化

```javascript
// 音频压缩和优化
class OptimizedVoiceInput {
    constructor() {
        this.audioContext = new AudioContext();
    }
    
    // 降采样到 16kHz
    async downsampleAudio(audioBlob) {
        const arrayBuffer = await audioBlob.arrayBuffer();
        const audioBuffer = await this.audioContext.decodeAudioData(arrayBuffer);
        
        // 创建离线音频上下文进行重采样
        const offlineContext = new OfflineAudioContext(
            1,  // 单声道
            audioBuffer.duration * 16000,  // 16kHz
            16000
        );
        
        const source = offlineContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(offlineContext.destination);
        source.start();
        
        const resampled = await offlineContext.startRendering();
        
        // 转换为 WAV
        return this.audioBufferToWav(resampled);
    }
    
    // AudioBuffer 转 WAV
    audioBufferToWav(buffer) {
        const length = buffer.length * buffer.numberOfChannels * 2 + 44;
        const arrayBuffer = new ArrayBuffer(length);
        const view = new DataView(arrayBuffer);
        const channels = [];
        let offset = 0;
        let pos = 0;
        
        // WAV 头部
        setUint32(0x46464952);  // "RIFF"
        setUint32(length - 8);  // file length - 8
        setUint32(0x45564157);  // "WAVE"
        
        setUint32(0x20746d66);  // "fmt " chunk
        setUint32(16);  // length = 16
        setUint16(1);  // PCM
        setUint16(buffer.numberOfChannels);
        setUint32(buffer.sampleRate);
        setUint32(buffer.sampleRate * 2 * buffer.numberOfChannels);  // avg. bytes/sec
        setUint16(buffer.numberOfChannels * 2);  // block-align
        setUint16(16);  // 16-bit
        
        setUint32(0x61746164);  // "data" chunk
        setUint32(length - pos - 4);  // chunk length
        
        // 写入 PCM 数据
        for (let i = 0; i < buffer.numberOfChannels; i++) {
            channels.push(buffer.getChannelData(i));
        }
        
        while (pos < length) {
            for (let i = 0; i < buffer.numberOfChannels; i++) {
                let sample = Math.max(-1, Math.min(1, channels[i][offset]));
                sample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
                view.setInt16(pos, sample, true);
                pos += 2;
            }
            offset++;
        }
        
        return new Blob([arrayBuffer], { type: 'audio/wav' });
        
        function setUint16(data) {
            view.setUint16(pos, data, true);
            pos += 2;
        }
        
        function setUint32(data) {
            view.setUint32(pos, data, true);
            pos += 4;
        }
    }
}
```

### 2. 用户体验优化

```javascript
// 带有 VAD（语音活动检测）的实现
class VoiceInputWithVAD {
    constructor() {
        this.audioContext = new AudioContext();
        this.analyser = null;
        this.silenceTimeout = null;
        this.silenceDuration = 2000;  // 2秒静音后自动停止
    }
    
    setupVAD(stream) {
        const source = this.audioContext.createMediaStreamSource(stream);
        this.analyser = this.audioContext.createAnalyser();
        this.analyser.fftSize = 256;
        
        source.connect(this.analyser);
        
        this.checkAudioLevel();
    }
    
    checkAudioLevel() {
        const dataArray = new Uint8Array(this.analyser.frequencyBinCount);
        this.analyser.getByteFrequencyData(dataArray);
        
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        
        if (average > 10) {  // 有声音
            this.clearSilenceTimeout();
            this.onSpeechDetected?.();
        } else {  // 静音
            if (!this.silenceTimeout) {
                this.silenceTimeout = setTimeout(() => {
                    this.onSilenceDetected?.();
                    this.stopRecording();
                }, this.silenceDuration);
            }
        }
        
        if (this.isRecording) {
            requestAnimationFrame(() => this.checkAudioLevel());
        }
    }
    
    clearSilenceTimeout() {
        if (this.silenceTimeout) {
            clearTimeout(this.silenceTimeout);
            this.silenceTimeout = null;
        }
    }
}
```

### 3. 安全性考虑

```javascript
// 安全的语音输入实现
class SecureVoiceInput {
    constructor() {
        this.config = {
            baseURL: this.getSecureBaseURL(),
            maxFileSize: 10 * 1024 * 1024,  // 10MB
            allowedFormats: ['audio/webm', 'audio/wav', 'audio/mp3'],
            csrfToken: this.getCSRFToken()
        };
    }
    
    getSecureBaseURL() {
        // 使用 HTTPS
        const url = process.env.SPEACHES_URL || 'http://localhost:8000';
        return url.replace('http://', 'https://');
    }
    
    getCSRFToken() {
        // 从 cookie 或 meta 标签获取 CSRF token
        return document.querySelector('meta[name="csrf-token"]')?.content || '';
    }
    
    async transcribe(audioBlob) {
        // 验证文件大小
        if (audioBlob.size > this.config.maxFileSize) {
            throw new Error('音频文件过大');
        }
        
        // 验证文件类型
        if (!this.config.allowedFormats.includes(audioBlob.type)) {
            throw new Error('不支持的音频格式');
        }
        
        const formData = new FormData();
        formData.append('file', audioBlob);
        formData.append('model', this.config.model);
        
        const response = await fetch(`${this.config.baseURL}/v1/audio/transcriptions`, {
            method: 'POST',
            headers: {
                'X-CSRF-Token': this.config.csrfToken
            },
            body: formData,
            credentials: 'same-origin'  // 包含 cookies
        });
        
        return response.json();
    }
}
```

## 部署注意事项

### 1. HTTPS 要求
- 浏览器要求 HTTPS 才能访问麦克风
- 本地开发可以使用 localhost

### 2. CORS 配置
```javascript
// 如果前端和后端不同域
const response = await fetch('https://api.example.com/v1/audio/transcriptions', {
    method: 'POST',
    mode: 'cors',
    credentials: 'include',
    headers: {
        'Origin': window.location.origin
    },
    body: formData
});
```

### 3. 代理配置（开发环境）
```javascript
// vite.config.js
export default {
  server: {
    proxy: {
      '/v1': {
        target: 'http://localhost:8000',
        changeOrigin: true
      }
    }
  }
}

// webpack.config.js
module.exports = {
  devServer: {
    proxy: {
      '/v1': 'http://localhost:8000'
    }
  }
}
```

## 测试和调试

### 1. 模拟音频输入
```javascript
// 用于测试的模拟音频
async function createTestAudio() {
    const audioContext = new AudioContext();
    const oscillator = audioContext.createOscillator();
    const destination = audioContext.createMediaStreamDestination();
    
    oscillator.connect(destination);
    oscillator.frequency.value = 440;  // A4 音符
    oscillator.start();
    
    setTimeout(() => oscillator.stop(), 1000);  // 1秒
    
    return destination.stream;
}
```

### 2. 调试工具
```javascript
// 语音输入调试器
class VoiceInputDebugger {
    constructor(voiceInput) {
        this.voiceInput = voiceInput;
        this.logs = [];
        
        this.interceptMethods();
        this.createDebugPanel();
    }
    
    interceptMethods() {
        const originalStart = this.voiceInput.startRecording;
        this.voiceInput.startRecording = async (...args) => {
            this.log('开始录音', { timestamp: Date.now() });
            return originalStart.apply(this.voiceInput, args);
        };
        
        const originalTranscribe = this.voiceInput.transcribe;
        this.voiceInput.transcribe = async (...args) => {
            const start = Date.now();
            try {
                const result = await originalTranscribe.apply(this.voiceInput, args);
                this.log('转录成功', { 
                    duration: Date.now() - start,
                    result: result.text 
                });
                return result;
            } catch (err) {
                this.log('转录失败', { 
                    duration: Date.now() - start,
                    error: err.message 
                });
                throw err;
            }
        };
    }
    
    log(message, data) {
        const entry = {
            time: new Date().toLocaleTimeString(),
            message,
            data
        };
        
        this.logs.push(entry);
        this.updateDebugPanel(entry);
    }
    
    createDebugPanel() {
        const panel = document.createElement('div');
        panel.id = 'voice-debug-panel';
        panel.style.cssText = `
            position: fixed;
            bottom: 20px;
            right: 20px;
            width: 300px;
            max-height: 400px;
            background: rgba(0, 0, 0, 0.8);
            color: white;
            padding: 10px;
            border-radius: 8px;
            font-family: monospace;
            font-size: 12px;
            overflow-y: auto;
            z-index: 9999;
        `;
        
        document.body.appendChild(panel);
    }
    
    updateDebugPanel(entry) {
        const panel = document.getElementById('voice-debug-panel');
        const line = document.createElement('div');
        line.textContent = `[${entry.time}] ${entry.message}`;
        
        if (entry.data) {
            const details = document.createElement('pre');
            details.textContent = JSON.stringify(entry.data, null, 2);
            details.style.marginLeft = '10px';
            line.appendChild(details);
        }
        
        panel.appendChild(line);
        panel.scrollTop = panel.scrollHeight;
    }
}

// 使用调试器
const voiceInput = new VoiceInput();
const debugger = new VoiceInputDebugger(voiceInput);
```

## 浏览器兼容性

### 支持的浏览器
- Chrome 25+
- Firefox 29+
- Safari 11+
- Edge 12+

### 兼容性检查
```javascript
function checkBrowserSupport() {
    const support = {
        getUserMedia: !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia),
        mediaRecorder: typeof MediaRecorder !== 'undefined',
        webAudio: typeof AudioContext !== 'undefined' || typeof webkitAudioContext !== 'undefined',
        webSocket: typeof WebSocket !== 'undefined'
    };
    
    const isSupported = Object.values(support).every(v => v);
    
    if (!isSupported) {
        console.warn('浏览器兼容性:', support);
    }
    
    return isSupported;
}
```

---

这个文档提供了完整的前端接入方案，包括非实时和实时两种模式，以及各种框架的实现示例。您可以根据具体需求选择合适的实现方式。